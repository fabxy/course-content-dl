{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "execution": {},
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeuromatchAcademy/course-content-dl/blob/main/projects/ReinforcementLearning/human_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "# Using RL to Model Cognitive Tasks\n",
    "\n",
    "**By Neurmatch Academy**\n",
    "\n",
    "__Content creators:__ Morteza Ansarinia, Yamil Vidal\n",
    "\n",
    "__Production editor:__ Spiros Chavlis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n",
    "\n",
    "<p align='center'><img src='https://github.com/NeuromatchAcademy/widgets/blob/master/sponsors.png?raw=True'/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Objective\n",
    "\n",
    "- This project aims to use behavioral data to train an agent and then use the agent to investigate data produced by human subjects. Having a computational agent that mimics humans in such tests, we will be able to compare its mechanics with human data.\n",
    "\n",
    "- In another conception, we could fit an agent that learns many cognitive tasks that require abstract-level constructs such as executive functions. This is a multi-task control problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |█▌                              | 10 kB 22.4 MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 20 kB 28.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████▌                           | 30 kB 12.9 MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 40 kB 9.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████▌                        | 51 kB 5.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 61 kB 5.8 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▌                     | 71 kB 5.6 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 81 kB 6.3 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 92 kB 6.3 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 102 kB 5.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▍               | 112 kB 5.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 122 kB 5.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▍            | 133 kB 5.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 143 kB 5.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▍         | 153 kB 5.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 163 kB 5.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 174 kB 5.1 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 184 kB 5.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 194 kB 5.1 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▉  | 204 kB 5.1 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 215 kB 5.1 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 219 kB 5.1 MB/s \n",
      "\u001b[?25h  Building wheel for dm-acme (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 14.7 MB 122 kB/s \n",
      "\u001b[K     |████████████████████████████████| 254 kB 5.0 MB/s \n",
      "\u001b[K     |████████████████████████████████| 99 kB 3.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 4.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 5.2 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# @title Install dependencies\n",
    "!pip install dm-env --quiet\n",
    "!pip install dm-acme --quiet\n",
    "!pip install dm-acme[reverb] --quiet\n",
    "!pip install dm-sonnet --quiet\n",
    "!pip install trfl --quiet\n",
    "!pip install dm-reverb --quiet\n",
    "!pip install dm-acme[launchpad] --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sonnet as snt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dm_env\n",
    "\n",
    "import acme\n",
    "from acme import specs\n",
    "from acme import wrappers\n",
    "from acme import EnvironmentLoop\n",
    "from acme.tf import networks\n",
    "from acme.testing import fakes\n",
    "from acme.agents.tf import dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "from IPython.display import clear_output, display, HTML\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# @title `InMemoryLogger` that keeps the data in memory\n",
    "\n",
    "class InMemoryLogger(acme.utils.loggers.Logger):\n",
    "  \"\"\"A simple logger that keeps all data in memory.\n",
    "\n",
    "  Reference:\n",
    "    https://github.com/deepmind/acme/blob/master/acme/utils/loggers/dataframe.py\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    self._data = []\n",
    "\n",
    "  def write(self, data: acme.utils.loggers.LoggingData):\n",
    "    self._data.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Background\n",
    "\n",
    "- Cognitive scientists use standard lab tests to tap into specific processes in the brain and behavior. Some examples of those tests are Stroop, N-back, Digit Span, TMT (Trail making tests), and WCST (Wisconsin Card Sorting Tests).\n",
    "\n",
    "- Despite an extensive body of research that explains human performance using descriptive what-models, we still need a more sophisticated approach to gain a better understanding of the underlying processes (i.e., a how-model).\n",
    "\n",
    "- Interestingly, many of such tests can be thought of as a continuous stream of stimuli and corresponding actions, that is in consonant with the RL formulation. In fact, RL itself is in part motivated by how the brain enables goal-directed behaviors using reward systems, making it a good choice to explain human performance.\n",
    "\n",
    "- One behavioral test example would be the N-back task.\n",
    "\n",
    "  - In the N-back, participants view a sequence of stimuli, one by one, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedback is given at both timestep and trajectory levels.\n",
    "\n",
    "  - The agent is rewarded when its response matches the stimulus that was shown N steps back in the episode. A simpler version of the N-back uses two-choice action schema, that is match vs non-match. Once the present stimulus matches the one presented N step back, then the agent is expected to respond to it as being a `match`.\n",
    "\n",
    "\n",
    "- Given a trained RL agent, we then find correlates of its fitted parameters with the brain mechanisms. The most straightforward composition could be the correlation of model parameters with the brain activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Datasets\n",
    "\n",
    "- HCP WM task ([NMA-CN HCP notebooks](https://github.com/NeuromatchAcademy/course-content/tree/master/projects/fMRI))\n",
    "\n",
    "Any dataset that used cognitive tests would work.\n",
    "Question: limit to behavioral data vs fMRI?\n",
    "Question: Which stimuli and actions to use?\n",
    "classic tests can be modeled using 1) bounded symbolic stimuli/actions (e.g., A, B, C), but more sophisticated one would require texts or images (e.g., face vs neutral images in social stroop dataset)\n",
    "The HCP dataset from NMA-CN contains behavioral and imaging data for 7 cognitive tests including various versions of N-back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## N-back task\n",
    "\n",
    "In the N-back task, participants view a sequence of stimuli, one per time, and are asked to categorize each stimulus as being either match or non-match. Stimuli are usually numbers, and feedbacks are given at both timestep and trajectory levels.\n",
    "\n",
    "In a typical neuro setup, both accuracy and response time are measured, but here, for the sake of brevity, we focus only on accuracy of responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "---\n",
    "# Cognitive Tests Environment\n",
    "\n",
    "First we develop an environment in that agents perform a cognitive test, here the N-back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Human dataset\n",
    "\n",
    "We need a dataset of human perfoming a N-back test, with the following features:\n",
    "\n",
    "- `participant_id`: following the BIDS format, it contains a unique identifier for each participant.\n",
    "- `trial_index`: same as `time_step`.\n",
    "- `stimulus`: same as `observation`.\n",
    "- `response`: same as `action`, recorded response by the human subject.\n",
    "- `expected_response`: correct response.\n",
    "- `is_correct`: same as `reward`, whether the human subject responded correctly.\n",
    "- `response_time`: won't be used here.\n",
    "\n",
    "Here we generate a mock dataset with those features, but remember to **replace this with real human data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n",
      "<string>:6: RuntimeWarning: Converting input from bool to <class 'numpy.uint8'> for compatibility.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>trial_index</th>\n",
       "      <th>stimulus</th>\n",
       "      <th>response</th>\n",
       "      <th>response_time</th>\n",
       "      <th>expected_response</th>\n",
       "      <th>is_correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-1</td>\n",
       "      <td>1</td>\n",
       "      <td>D</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-1</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-1</td>\n",
       "      <td>3</td>\n",
       "      <td>B</td>\n",
       "      <td>non-match</td>\n",
       "      <td>0.140398</td>\n",
       "      <td>non-match</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-1</td>\n",
       "      <td>4</td>\n",
       "      <td>A</td>\n",
       "      <td>match</td>\n",
       "      <td>1.317116</td>\n",
       "      <td>match</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-1</td>\n",
       "      <td>5</td>\n",
       "      <td>F</td>\n",
       "      <td>match</td>\n",
       "      <td>0.328139</td>\n",
       "      <td>non-match</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  participant_id  trial_index  ... expected_response is_correct\n",
       "0          sub-1            1  ...              None       True\n",
       "1          sub-1            2  ...              None       True\n",
       "2          sub-1            3  ...         non-match       True\n",
       "3          sub-1            4  ...             match       True\n",
       "4          sub-1            5  ...         non-match      False\n",
       "\n",
       "[5 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_mock_nback_dataset(N=2,\n",
    "                                n_participants=10,\n",
    "                                n_trials=32,\n",
    "                                stimulus_choices=list('ABCDEF'),\n",
    "                                response_choices=['match', 'non-match']):\n",
    "  \"\"\"Generate a mock dataset for the N-back task.\"\"\"\n",
    "\n",
    "  n_rows = n_participants * n_trials\n",
    "\n",
    "  participant_ids = sorted([f'sub-{pid}' for pid in range(1,n_participants+1)] * n_trials)\n",
    "  trial_indices = list(range(1,n_trials+1)) * n_participants\n",
    "  stimulus_sequence = np.random.choice(stimulus_choices, n_rows)\n",
    "\n",
    "  responses = np.random.choice(response_choices, n_rows)\n",
    "  response_times = np.random.exponential(size=n_rows)\n",
    "\n",
    "  df = pd.DataFrame({\n",
    "      'participant_id': participant_ids,\n",
    "      'trial_index': trial_indices,\n",
    "      'stimulus': stimulus_sequence,\n",
    "      'response': responses,\n",
    "      'response_time': response_times\n",
    "  })\n",
    "\n",
    "  # mark matchig stimuli\n",
    "  _nback_stim = df['stimulus'].shift(N)\n",
    "  df['expected_response'] = (df['stimulus'] == _nback_stim).map({True: 'match', False: 'non-match'})\n",
    "\n",
    "  df['is_correct'] = (df['response'] == df['expected_response'])\n",
    "\n",
    "  # we don't care about burn-in trials (trial < N)\n",
    "  df.loc[df['trial_index'] <= N, 'is_correct'] = True\n",
    "  df.loc[df['trial_index'] <= N, ['response','response_time','expected_response']] = None\n",
    "\n",
    "  return df\n",
    "\n",
    "\n",
    "# ========\n",
    "# now generate the actual data with the provided function and plot some of its features\n",
    "mock_nback_data = generate_mock_nback_dataset()\n",
    "\n",
    "sns.displot(data=mock_nback_data, x='response_time')\n",
    "plt.suptitle('response time distribution of the mock N-back dataset', y=1.01)\n",
    "plt.show()\n",
    "\n",
    "sns.displot(data=mock_nback_data, x='is_correct')\n",
    "plt.suptitle('Accuracy distribution of the mock N-back dataset', y=1.06)\n",
    "plt.show()\n",
    "\n",
    "mock_nback_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "## Implementation scheme\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Environment\n",
    "\n",
    "The following cell implments N-back envinronment, that we later use to train a RL agent on human data. It is capable of performing two kinds of simulation:\n",
    "- rewards the agent once the action was correct (i.e., a normative model of the environment).\n",
    "- receives human data (or mock data if you prefer), and returns what participants performed as the observation. This is more useful for preference-based RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class NBack(dm_env.Environment):\n",
    "\n",
    "  ACTIONS = ['match', 'non-match']\n",
    "\n",
    "  def __init__(self,\n",
    "               N=2,\n",
    "               episode_steps=32,\n",
    "               stimuli_choices=list('ABCDEF'),\n",
    "               human_data=None,\n",
    "               seed=1,\n",
    "               ):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      N: Number of steps to look back for the matched stimuli. Defaults to 2 (as in 2-back).\n",
    "      episode_steps\n",
    "      stimuli_choices\n",
    "      human_data\n",
    "      seed\n",
    "\n",
    "    \"\"\"\n",
    "    self.N = N\n",
    "    self.episode_steps = episode_steps\n",
    "    self.stimuli_choices = stimuli_choices\n",
    "    self.stimuli = np.empty(shape=episode_steps)  # will be filled in the `reset()`\n",
    "\n",
    "    self._reset_next_step = True\n",
    "\n",
    "    # whether mimic humans or reward the agent once it responds optimally.\n",
    "    if human_data is None:\n",
    "      self._imitate_human = False\n",
    "      self.human_data = None\n",
    "      self.human_subject_data = None\n",
    "    else:\n",
    "      self._imitate_human = True\n",
    "      self.human_data = human_data\n",
    "      self.human_subject_data = None\n",
    "\n",
    "    self._action_history = []\n",
    "\n",
    "  def reset(self):\n",
    "    self._reset_next_step = False\n",
    "    self._current_step = 0\n",
    "    self._action_history.clear()\n",
    "\n",
    "    # generate a random sequence instead of relying on human data\n",
    "    if self.human_data is None:\n",
    "      # self.stimuli = np.random.choice(self.stimuli_choices, self.episode_steps)\n",
    "      # FIXME This is a fix for acme & reverb issue with string observation. Agent should be able to handle strings\n",
    "      self.stimuli = np.random.choice(len(self.stimuli_choices), self.episode_steps).astype(np.float32)\n",
    "    else:\n",
    "      # randomly choose a subject from the human data and follow her trials and responses.\n",
    "      self.human_subject_data = self.human_data.query('participant_id == participant_id.sample().iloc[0]',\n",
    "                                                engine='python').sort_values('trial_index')\n",
    "      self.stimuli = self.human_subject_data['stimulus'].values\n",
    "      # FIXME should we always use one specific human subject or randomly select one in each episode?\n",
    "\n",
    "    return dm_env.restart(self._observation())\n",
    "\n",
    "\n",
    "  def _episode_return(self):\n",
    "    if self._imitate_human:\n",
    "      return np.mean(self.human_subject_data['response'] == self._action_history)\n",
    "    else:\n",
    "      return 0.0\n",
    "\n",
    "  def step(self, action: int):\n",
    "    if self._reset_next_step:\n",
    "      return self.reset()\n",
    "\n",
    "    agent_action = NBack.ACTIONS[action]\n",
    "\n",
    "    if self._imitate_human:\n",
    "      # if it was the same action as the human subject, then reward the agent\n",
    "      human_action = self.human_subject_data['response'].iloc[self._current_step]\n",
    "      step_reward = (agent_action == human_action)\n",
    "    else:\n",
    "      # assume the agent is rationale and doesn't want to reproduce human, reward once the response it correct\n",
    "      expected_action = 'match' if (self.stimuli[self._current_step] == self.stimuli[self._current_step - self.N]) else 'non-match'\n",
    "      step_reward = 0. if (agent_action == expected_action) else -1.\n",
    "\n",
    "    self._action_history.append(agent_action)\n",
    "\n",
    "    self._current_step += 1\n",
    "\n",
    "    # Check for termination.\n",
    "    if self._current_step == self.stimuli.shape[0]:\n",
    "      self._reset_next_step = True\n",
    "      # we are using the mean of total time step rewards as the episode return\n",
    "      return dm_env.termination(reward=self._episode_return(),\n",
    "                                observation=self._observation())\n",
    "    else:\n",
    "      return dm_env.transition(reward=step_reward,\n",
    "                               observation=self._observation())\n",
    "\n",
    "  def observation_spec(self):\n",
    "    return dm_env.specs.BoundedArray(\n",
    "        shape=self.stimuli.shape,\n",
    "        dtype=self.stimuli.dtype,\n",
    "        name='nback_stimuli', minimum=0, maximum=self.stimuli.max())\n",
    "\n",
    "  def action_spec(self):\n",
    "    return dm_env.specs.DiscreteArray(\n",
    "        num_values=len(NBack.ACTIONS),\n",
    "        dtype=np.int32,\n",
    "        name='action')\n",
    "\n",
    "  def _observation(self):\n",
    "\n",
    "    # agent observes only the current trial\n",
    "    # obs = self.stimuli[self._current_step - 1]\n",
    "\n",
    "    # agents observe stimuli up to the current trial\n",
    "    obs = self.stimuli[:self._current_step].copy()\n",
    "    obs = np.pad(obs,(0, len(self.stimuli) - len(obs)))\n",
    "\n",
    "    return obs\n",
    "\n",
    "  def plot_state(self):\n",
    "    \"\"\"Display current state of the environment.\n",
    "\n",
    "     Note: `M` mean `match`, and `.` is a `non-match`.\n",
    "    \"\"\"\n",
    "    stimuli = self.stimuli[:self._current_step - 1]\n",
    "    actions = ['M' if a=='match' else '.' for a in self._action_history[:self._current_step - 1]]\n",
    "    return HTML(\n",
    "        f'<b>Environment ({self.N}-back):</b><br />'\n",
    "        f'<pre><b>Stimuli:</b> {\"\".join(map(str,map(int,stimuli)))}</pre>'\n",
    "        f'<pre><b>Actions:</b> {\"\".join(actions)}</pre>'\n",
    "    )\n",
    "\n",
    "  @staticmethod\n",
    "  def create_environment():\n",
    "    \"\"\"Utility function to create a N-back environment and its spec.\"\"\"\n",
    "\n",
    "    # Make sure the environment outputs single-precision floats.\n",
    "    environment = wrappers.SinglePrecisionWrapper(NBack())\n",
    "\n",
    "    # Grab the spec of the environment.\n",
    "    environment_spec = specs.make_environment_spec(environment)\n",
    "\n",
    "    return environment, environment_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Define a random agent\n",
    "\n",
    "For more information you can refer to NMA-DL W3D2 Basic Reinforcement learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "class RandomAgent(acme.Actor):\n",
    "\n",
    "  def __init__(self, environment_spec):\n",
    "    \"\"\"Gets the number of available actions from the environment spec.\"\"\"\n",
    "    self._num_actions = environment_spec.actions.num_values\n",
    "\n",
    "  def select_action(self, observation):\n",
    "    \"\"\"Selects an action uniformly at random.\"\"\"\n",
    "    action = np.random.randint(self._num_actions)\n",
    "    return action\n",
    "\n",
    "  def observe_first(self, timestep):\n",
    "    \"\"\"Does not record as the RandomAgent has no use for data.\"\"\"\n",
    "    pass\n",
    "\n",
    "  def observe(self, action, next_timestep):\n",
    "    \"\"\"Does not record as the RandomAgent has no use for data.\"\"\"\n",
    "    pass\n",
    "\n",
    "  def update(self):\n",
    "    \"\"\"Does not update as the RandomAgent does not learn from data.\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Initialize the environment and the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions:\n",
      " DiscreteArray(shape=(), dtype=int32, name=action, minimum=0, maximum=1, num_values=2)\n",
      "observations:\n",
      " BoundedArray(shape=(32,), dtype=dtype('float32'), name='nback_stimuli', minimum=0.0, maximum=nan)\n",
      "rewards:\n",
      " Array(shape=(), dtype=dtype('float32'), name='reward')\n"
     ]
    }
   ],
   "source": [
    "env, env_spec = NBack.create_environment()\n",
    "agent = RandomAgent(env_spec)\n",
    "\n",
    "print('actions:\\n', env_spec.actions)\n",
    "print('observations:\\n', env_spec.observations)\n",
    "print('rewards:\\n', env_spec.rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "### Run the loop\n",
    "\n",
    "For more details, see NMA-DL W3D2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " All episode returns: [-16.0, -14.0, -14.0, -12.0, -16.0, -13.0, -14.0, -14.0, -16.0, -18.0, -17.0, -14.0, -19.0, -17.0, -21.0, -15.0, -12.0, -17.0, -15.0, -12.0, -20.0, -14.0, -20.0, -18.0, -16.0, -17.0, -17.0, -18.0, -20.0, -10.0, -18.0, -14.0, -13.0, -12.0, -20.0, -14.0, -15.0, -20.0, -20.0, -17.0, -17.0, -13.0, -16.0, -13.0, -11.0, -16.0, -15.0, -14.0, -18.0, -12.0, -12.0, -20.0, -14.0, -17.0, -20.0, -17.0, -16.0, -12.0, -14.0, -11.0, -14.0, -18.0, -21.0, -18.0, -16.0, -16.0, -11.0, -17.0, -18.0, -14.0, -14.0, -10.0, -13.0, -17.0, -14.0, -18.0, -15.0, -17.0, -12.0, -19.0, -15.0, -20.0, -14.0, -17.0, -12.0, -13.0, -9.0, -17.0, -17.0, -15.0, -16.0, -19.0, -16.0, -19.0, -19.0, -14.0, -16.0, -11.0, -21.0, -14.0, -17.0, -19.0, -15.0, -14.0, -10.0, -17.0, -22.0, -12.0, -15.0, -20.0, -18.0, -14.0, -20.0, -17.0, -15.0, -16.0, -18.0, -11.0, -12.0, -15.0, -13.0, -20.0, -17.0, -11.0, -19.0, -15.0, -17.0, -18.0, -17.0, -11.0, -12.0, -16.0, -16.0, -14.0, -10.0, -18.0, -15.0, -18.0, -17.0, -17.0, -15.0, -21.0, -14.0, -16.0, -14.0, -10.0, -16.0, -15.0, -14.0, -11.0, -15.0, -18.0, -16.0, -17.0, -19.0, -20.0, -14.0, -16.0, -16.0, -13.0, -15.0, -14.0, -14.0, -16.0, -18.0, -19.0, -15.0, -18.0, -16.0, -10.0, -16.0, -17.0, -14.0, -16.0, -19.0, -21.0, -16.0, -18.0, -15.0, -14.0, -16.0, -13.0, -13.0, -11.0, -13.0, -13.0, -14.0, -16.0, -12.0, -17.0, -17.0, -15.0, -16.0, -16.0, -17.0, -17.0, -18.0, -10.0, -14.0, -18.0, -16.0, -14.0, -15.0, -16.0, -15.0, -18.0, -14.0, -16.0, -16.0, -11.0, -16.0, -12.0, -16.0, -14.0, -18.0, -14.0, -15.0, -15.0, -15.0, -14.0, -16.0, -16.0, -9.0, -16.0, -15.0, -12.0, -14.0, -12.0, -13.0, -17.0, -12.0, -21.0, -18.0, -17.0, -16.0, -21.0, -19.0, -15.0, -14.0, -13.0, -17.0, -13.0, -13.0, -22.0, -17.0, -11.0, -18.0, -17.0, -16.0, -12.0, -10.0, -16.0, -14.0, -18.0, -11.0, -13.0, -13.0, -11.0, -14.0, -14.0, -17.0, -14.0, -16.0, -9.0, -16.0, -8.0, -16.0, -14.0, -17.0, -14.0, -21.0, -15.0, -17.0, -20.0, -20.0, -13.0, -13.0, -17.0, -19.0, -14.0, -15.0, -13.0, -16.0, -19.0, -16.0, -12.0, -17.0, -16.0, -13.0, -16.0, -11.0, -16.0, -19.0, -18.0, -13.0, -16.0, -18.0, -15.0, -20.0, -14.0, -11.0, -16.0, -18.0, -15.0, -22.0, -17.0, -15.0, -18.0, -14.0, -16.0, -17.0, -17.0, -11.0, -17.0, -21.0, -15.0, -15.0, -13.0, -18.0, -15.0, -16.0, -15.0, -23.0, -10.0, -10.0, -22.0, -13.0, -16.0, -16.0, -15.0, -10.0, -13.0, -16.0, -16.0, -12.0, -16.0, -19.0, -16.0, -18.0, -13.0, -15.0, -15.0, -11.0, -17.0, -17.0, -9.0, -12.0, -11.0, -13.0, -18.0, -13.0, -16.0, -14.0, -14.0, -16.0, -20.0, -20.0, -10.0, -15.0, -20.0, -17.0, -15.0, -14.0, -18.0, -20.0, -14.0, -12.0, -13.0, -14.0, -20.0, -16.0, -14.0, -18.0, -18.0, -19.0, -20.0, -14.0, -12.0, -15.0, -18.0, -19.0, -18.0, -20.0, -12.0, -14.0, -13.0, -13.0, -17.0, -15.0, -12.0, -12.0, -16.0, -13.0, -12.0, -16.0, -16.0, -17.0, -13.0, -18.0, -17.0, -14.0, -14.0, -20.0, -17.0, -23.0, -17.0, -13.0, -7.0, -10.0, -19.0, -17.0, -14.0, -12.0, -17.0, -18.0, -16.0, -16.0, -14.0, -10.0, -13.0, -14.0, -14.0, -15.0, -16.0, -16.0, -12.0, -16.0, -17.0, -12.0, -11.0, -16.0, -14.0, -13.0, -17.0, -12.0, -14.0, -16.0, -17.0, -15.0, -12.0, -11.0, -14.0, -18.0, -19.0, -11.0, -19.0, -13.0, -15.0, -15.0, -13.0, -18.0, -12.0, -13.0, -21.0, -17.0, -10.0, -17.0, -20.0, -10.0, -17.0, -14.0, -14.0, -12.0, -17.0, -15.0, -12.0, -15.0, -18.0, -10.0, -16.0, -17.0, -17.0, -18.0, -14.0, -19.0, -12.0, -14.0, -10.0, -14.0, -15.0, -19.0, -11.0, -16.0, -13.0, -19.0, -19.0, -16.0, -18.0, -13.0, -14.0, -18.0, -7.0, -20.0, -16.0, -15.0, -14.0, -16.0, -18.0, -12.0, -18.0, -15.0, -20.0, -15.0, -15.0, -13.0, -14.0, -15.0, -16.0, -15.0, -16.0, -13.0, -17.0, -15.0, -17.0, -14.0, -15.0, -18.0, -9.0, -16.0, -7.0, -9.0, -12.0, -18.0, -10.0, -14.0, -12.0, -16.0, -19.0, -16.0, -15.0, -16.0, -18.0, -10.0, -15.0, -20.0, -19.0, -11.0, -15.0, -19.0, -14.0, -16.0, -19.0, -15.0, -15.0, -15.0, -17.0, -17.0, -15.0, -18.0, -17.0, -14.0, -19.0, -15.0, -15.0, -17.0, -10.0, -16.0, -11.0, -13.0, -14.0, -12.0, -16.0, -17.0, -10.0, -15.0, -17.0, -19.0, -16.0, -19.0, -19.0, -17.0, -15.0, -16.0, -16.0, -17.0, -15.0, -18.0, -15.0, -13.0, -20.0, -16.0, -20.0, -14.0, -16.0, -11.0, -17.0, -14.0, -15.0, -19.0, -18.0, -18.0, -15.0, -12.0, -14.0, -20.0, -13.0, -10.0, -13.0, -11.0, -19.0, -16.0, -16.0, -19.0, -14.0, -15.0, -18.0, -18.0, -17.0, -16.0, -18.0, -14.0, -17.0, -12.0, -15.0, -10.0, -15.0, -17.0, -19.0, -12.0, -14.0, -18.0, -15.0, -15.0, -13.0, -13.0, -22.0, -18.0, -17.0, -9.0, -17.0, -10.0, -18.0, -15.0, -10.0, -13.0, -14.0, -17.0, -20.0, -16.0, -18.0, -15.0, -15.0, -17.0, -14.0, -17.0, -16.0, -17.0, -12.0, -20.0, -15.0, -16.0, -12.0, -16.0, -17.0, -18.0, -17.0, -15.0, -14.0, -10.0, -9.0, -17.0, -15.0, -19.0, -16.0, -15.0, -14.0, -16.0, -15.0, -16.0, -16.0, -13.0, -22.0, -20.0, -15.0, -14.0, -21.0, -16.0, -18.0, -12.0, -9.0, -11.0, -12.0, -16.0, -14.0, -15.0, -15.0, -14.0, -14.0, -13.0, -17.0, -15.0, -13.0, -20.0, -14.0, -15.0, -19.0, -15.0, -16.0, -23.0, -16.0, -15.0, -18.0, -17.0, -14.0, -17.0, -12.0, -16.0, -14.0, -13.0, -15.0, -15.0, -15.0, -15.0, -18.0, -13.0, -18.0, -13.0, -15.0, -20.0, -13.0, -15.0, -15.0, -18.0, -12.0, -15.0, -19.0, -18.0, -17.0, -17.0, -19.0, -20.0, -16.0, -16.0, -19.0, -12.0, -17.0, -15.0, -14.0, -15.0, -20.0, -13.0, -14.0, -14.0, -18.0, -14.0, -14.0, -14.0, -15.0, -11.0, -14.0, -18.0, -15.0, -13.0, -17.0, -19.0, -15.0, -9.0, -19.0, -18.0, -16.0, -18.0, -14.0, -17.0, -17.0, -9.0, -22.0, -17.0, -11.0, -16.0, -12.0, -19.0, -12.0, -12.0, -20.0, -16.0, -15.0, -14.0, -18.0, -16.0, -16.0, -15.0, -17.0, -18.0, -15.0, -17.0, -14.0, -15.0, -19.0, -18.0, -18.0, -15.0, -15.0, -15.0, -16.0, -15.0, -14.0, -17.0, -14.0, -10.0, -14.0, -15.0, -18.0, -16.0, -14.0, -13.0, -12.0, -18.0, -22.0, -18.0, -18.0, -17.0, -14.0, -16.0, -17.0, -17.0, -13.0, -17.0, -19.0, -13.0, -17.0, -13.0, -22.0, -10.0, -17.0, -18.0, -19.0, -15.0, -17.0, -20.0, -16.0, -14.0, -17.0, -19.0, -13.0, -9.0, -16.0, -14.0, -18.0, -18.0, -18.0, -14.0, -13.0, -16.0, -17.0, -21.0, -12.0, -16.0, -15.0, -11.0, -17.0, -15.0, -16.0, -13.0, -19.0, -16.0, -12.0, -14.0, -19.0, -18.0, -17.0, -10.0, -10.0, -17.0, -12.0, -15.0, -18.0, -20.0, -18.0, -17.0, -16.0, -13.0, -12.0, -17.0, -13.0, -14.0, -15.0, -22.0, -14.0, -17.0, -19.0, -10.0, -13.0, -17.0, -15.0, -17.0, -18.0, -19.0, -16.0, -14.0, -16.0, -15.0, -13.0, -18.0, -18.0, -16.0, -17.0, -15.0, -16.0, -16.0, -17.0, -12.0, -14.0, -18.0, -17.0, -15.0, -15.0, -16.0, -13.0, -22.0, -18.0, -18.0, -11.0, -9.0, -13.0, -16.0, -10.0, -13.0, -17.0, -14.0, -14.0, -10.0, -16.0, -12.0, -20.0, -16.0, -15.0, -13.0, -13.0, -16.0, -16.0, -9.0, -11.0, -16.0, -16.0, -14.0, -18.0, -17.0, -12.0, -16.0, -17.0, -17.0, -15.0, -16.0, -22.0, -16.0, -19.0, -14.0, -13.0, -16.0, -15.0, -15.0, -11.0, -16.0, -20.0, -16.0, -20.0, -15.0, -18.0, -13.0, -20.0, -14.0, -20.0, -13.0, -16.0, -14.0, -12.0, -16.0, -15.0, -13.0, -15.0, -12.0, -12.0, -14.0, -14.0, -12.0, -15.0, -14.0, -17.0, -13.0, -17.0, -19.0, -20.0, -13.0, -14.0, -11.0, -22.0, -16.0, -11.0, -13.0, -17.0, -14.0, -12.0, -12.0, -13.0, -15.0, -15.0, -22.0, -13.0, -20.0, -19.0]\n"
     ]
    }
   ],
   "source": [
    "# fitting parameters\n",
    "n_episodes = 1_000\n",
    "n_total_steps = 0\n",
    "log_loss = False\n",
    "n_steps = n_episodes * 32\n",
    "all_returns = []\n",
    "\n",
    "\n",
    "# main loop\n",
    "for episode in range(n_episodes):\n",
    "  episode_steps = 0\n",
    "  episode_return = 0\n",
    "  episode_loss = 0\n",
    "\n",
    "  start_time = time.time()\n",
    "\n",
    "  timestep = env.reset()\n",
    "\n",
    "  # Make the first observation.\n",
    "  agent.observe_first(timestep)\n",
    "\n",
    "  # Run an episode\n",
    "  while not timestep.last():\n",
    "\n",
    "    # DEBUG\n",
    "    # print(timestep)\n",
    "\n",
    "    # Generate an action from the agent's policy and step the environment.\n",
    "    action = agent.select_action(timestep.observation)\n",
    "    timestep = env.step(action)\n",
    "\n",
    "    # Have the agent observe the timestep and let the agent update itself.\n",
    "    agent.observe(action, next_timestep=timestep)\n",
    "    agent.update()\n",
    "\n",
    "    # Book-keeping.\n",
    "    episode_steps += 1\n",
    "    n_total_steps += 1\n",
    "    episode_return += timestep.reward\n",
    "\n",
    "    if log_loss:\n",
    "      episode_loss += agent.last_loss\n",
    "\n",
    "    if n_steps is not None and n_total_steps >= n_steps:\n",
    "      break\n",
    "\n",
    "  # Collect the results and combine with counts.\n",
    "  steps_per_second = episode_steps / (time.time() - start_time)\n",
    "  result = {\n",
    "      'episode': episode,\n",
    "      'episode_length': episode_steps,\n",
    "      'episode_return': episode_return,\n",
    "  }\n",
    "  if log_loss:\n",
    "    result['loss_avg'] = episode_loss/episode_steps\n",
    "\n",
    "  all_returns.append(episode_return)\n",
    "\n",
    "  display(env.plot_state())\n",
    "  # Log the given results.\n",
    "  print(result)\n",
    "\n",
    "  if n_steps is not None and n_total_steps >= n_steps:\n",
    "    break\n",
    "\n",
    "clear_output()\n",
    "print('\\n', 'All episode returns:', all_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "**Note:** You can simplify the environment loop using [DeepMind Acme](https://github.com/deepmind/acme)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "# init a new N-back environment\n",
    "env, env_spec = NBack.create_environment()\n",
    "\n",
    "# DEBUG fake testing environment.\n",
    "# Uncomment this to debug your agent without using the N-back environment.\n",
    "# env = fakes.DiscreteEnvironment(\n",
    "#     num_actions=2,\n",
    "#     num_observations=1000,\n",
    "#     obs_dtype=np.float32,\n",
    "#     episode_length=32)\n",
    "# env_spec = specs.make_environment_spec(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [],
   "source": [
    "def dqn_make_network(action_spec: specs.DiscreteArray) -> snt.Module:\n",
    "  return snt.Sequential([\n",
    "      snt.Flatten(),\n",
    "      snt.nets.MLP([50, 50, action_spec.num_values]),\n",
    "  ])\n",
    "\n",
    "# construct a DQN agent\n",
    "agent = dqn.DQN(\n",
    "    environment_spec=env_spec,\n",
    "    network=dqn_make_network(env_spec.actions),\n",
    "    epsilon=[0.5],\n",
    "    logger=InMemoryLogger(),\n",
    "    checkpoint=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {}
   },
   "source": [
    "Now, we run the environment loop with the DQN agent and print the training log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_length</th>\n",
       "      <th>episode_return</th>\n",
       "      <th>steps_per_second</th>\n",
       "      <th>episodes</th>\n",
       "      <th>steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>32</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>159.790428</td>\n",
       "      <td>996</td>\n",
       "      <td>31872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>32</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>155.916286</td>\n",
       "      <td>997</td>\n",
       "      <td>31904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>32</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>157.433133</td>\n",
       "      <td>998</td>\n",
       "      <td>31936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>32</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>169.210662</td>\n",
       "      <td>999</td>\n",
       "      <td>31968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>32</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>156.631546</td>\n",
       "      <td>1000</td>\n",
       "      <td>32000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     episode_length episode_return  steps_per_second  episodes  steps\n",
       "995              32          -16.0        159.790428       996  31872\n",
       "996              32          -10.0        155.916286       997  31904\n",
       "997              32          -12.0        157.433133       998  31936\n",
       "998              32          -16.0        169.210662       999  31968\n",
       "999              32          -12.0        156.631546      1000  32000"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training loop\n",
    "loop = EnvironmentLoop(env, agent, logger=InMemoryLogger())\n",
    "loop.run(n_episodes)\n",
    "\n",
    "# print logs\n",
    "logs = pd.DataFrame(loop._logger._data)\n",
    "logs.tail()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "human_rl",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
